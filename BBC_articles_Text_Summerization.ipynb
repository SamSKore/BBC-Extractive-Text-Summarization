{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BBC_articles_Text_Summerization.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RsFjRpvqkCc",
        "colab_type": "text"
      },
      "source": [
        "##Importing the files from kaggle and unzipping them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNfyeGGDrEDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg5kBS11rrkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /root/.kaggle\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "!cp /content/drive/\"My Drive\"/Codes/kaggle.json /root/.kaggle\n",
        "\n",
        "!kaggle datasets download -d pariza/bbc-news-summary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0z-0bbUsQf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/bbc-news-summary.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yTNxbL1sVnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "#!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT8k32ryRYyQ",
        "colab_type": "text"
      },
      "source": [
        "This project is about building the Automatic Extractive Text Summarization System.\n",
        "The dataset for this purpose has been taken from https://www.kaggle.com/pariza/bbc-news-summary\n",
        "\n",
        "This dataset for extractive text summarization has four hundred and seventeen political news articles of BBC from 2004 to 2005 in the News Articles folder.\n",
        "\n",
        "The dataset consists of 5 categories of news articles namely Business, Entertainment, Politics, Sports, Tech. We will build and test our model on Business news articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uaaaAzDW-SN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "all_files = os.listdir(\"/content/BBC News Summary/News Articles/business/\")\n",
        "print(all_files)\n",
        "\n",
        "business_news_articles = []\n",
        "\n",
        "for file_name in all_files:\n",
        "    name = \"/content/BBC News Summary/News Articles/business/\" + str(file_name)\n",
        "    with open(name, 'r') as f:\n",
        "        business_news_articles.append(f.read())\n",
        "\n",
        "df_articles = pd.DataFrame(business_news_articles)\n",
        "df_articles.columns = ['article']\n",
        "df_articles.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPMRnfmI4Z_1",
        "colab_type": "text"
      },
      "source": [
        "After importing all the articles from the text files under Business folder, we preprocess the data by doing the following:\n",
        "\n",
        "1. Removing Extra New Lines\n",
        "\n",
        "2. Removing punctuations and Numbers and special characters.\n",
        "\n",
        "3. Removing the Stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIemQu9xZmB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_newlines(text):\n",
        "    newlines_removed = text.replace('\\n', ' ')\n",
        "    \n",
        "    return newlines_removed;\n",
        "\n",
        "df_articles['lines_removed'] = df_articles['article'].apply(remove_newlines)\n",
        "\n",
        "df_articles.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnWRfvrVb788",
        "colab_type": "text"
      },
      "source": [
        "exploring the data:\n",
        "\n",
        "we first visualise the word count distribution with histotgram.\n",
        "\n",
        "after removing stopwords, unigram distribution can be shown. top 10 or 20 unigrams....\n",
        "\n",
        "top 20 bigrams can be shown.\n",
        "\n",
        "we can show the distribution of lengths of the articles.\n",
        "\n",
        "word clouds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGkBV5hagF6V",
        "colab_type": "text"
      },
      "source": [
        "we can also visualize the distribution of articles across GPEs present in those. like top 10 GPEs mentioned across all the articles.\n",
        "we'll get to know that BBC has published news articles preferrably related to specific regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH_LXZPp1nML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "punctuations = '!\"#$%^&*{`|}~<=>?;:/+-,@[\\_]'\n",
        "\n",
        "def clean_articles(article):\n",
        "    toks = nlp(article, disable=['tagger', 'parser', 'ner'])\n",
        "    punc_removed = [token.text for token in toks if token.text not in punctuations]\n",
        "    words = [word.lower() for word in punc_removed]    \n",
        "    # remove tokens with numbers in them\n",
        "    words = [word for word in words if word.isalpha() or word == '.']\n",
        "    \n",
        "    return ' '.join(words);\n",
        "    \n",
        "\n",
        "df_articles['cleaned_up_text'] = df_articles['lines_removed'].apply(clean_articles)\n",
        "\n",
        "\n",
        "df_articles['article_length'] = [len(article.split()) for article in df_articles['cleaned_up_text']]\n",
        "\n",
        "df_articles.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl1vDDItRjUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting Distribution of Article Lengths\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Article Lengths')\n",
        "plt.hist(df_articles['article_length'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKQf8wkE14DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing stopwords and unigram/bigram distributions\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "\n",
        "def remove_stops(text):\n",
        "    stops_removed = []\n",
        "    doc = nlp(text, disable=['parser','tagger'])\n",
        "    tokens = [token.text for token in doc if token.text not in stops]\n",
        "    stops_removed = ' '.join(tokens)\n",
        "    \n",
        "    return stops_removed;\n",
        "\n",
        "df_articles['stops_removed'] = df_articles['cleaned_up_text'].apply(remove_stops)\n",
        "\n",
        "df_articles.head(10)\n",
        "df_articles['stops_removed'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OM0PwRP9Hjw",
        "colab_type": "text"
      },
      "source": [
        "After Preprocessing, we'll do data exploration.\n",
        "\n",
        "We will have a look at distribution of Article lengths.\n",
        "\n",
        "We see that major number of articles are of the length between  200-300 words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf_BisIQ-BCH",
        "colab_type": "text"
      },
      "source": [
        "We will further observe the words appearing in the articles by visualizing the Top Unigrams, Bigrams and Trigrams distributions.\n",
        "\n",
        "Upon observing the top ngrams present across the articles, it is clear that most of those are Business related articles.\n",
        "\n",
        "We can also observe the Word CLoud and see the words appearing the most."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-nshZyJGHYI",
        "colab_type": "text"
      },
      "source": [
        "##Vizualizing the n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeQnwQhZ18Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "unigrams_frequencies = Counter([])\n",
        "bigrams_frequencies = Counter([])\n",
        "trigrams_frequencies = Counter([])\n",
        "\n",
        "def generate_ngrams():\n",
        "    global unigrams_frequencies, bigrams_frequencies, trigrams_frequencies;\n",
        "    for tex in df_articles['stops_removed']:\n",
        "        toks = nlp(tex, disable=['parser', 'tagger'])\n",
        "        tok_texts = [t.text for t in toks if t.text != '.']\n",
        "        unigrams = ngrams(tok_texts, 1)\n",
        "        bigrams = ngrams(tok_texts, 2)\n",
        "        trigrams = ngrams(tok_texts, 3)\n",
        "        bigrams_frequencies += Counter(bigrams)\n",
        "        unigrams_frequencies += Counter(unigrams)\n",
        "        trigrams_frequencies += Counter(trigrams)\n",
        "\n",
        "    return;\n",
        "    \n",
        "generate_ngrams()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOGKOQD92FXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "top_unigrams = unigrams_frequencies.most_common(20)\n",
        "top_bigrams = bigrams_frequencies.most_common(20)\n",
        "top_trigrams = trigrams_frequencies.most_common(20)\n",
        "\n",
        "#unzipping the ngrams\n",
        "strings_unigrams, count_unigrams = zip(*top_unigrams)\n",
        "strings_bigrams, count_bigrams = zip(*top_bigrams)\n",
        "strings_trigrams, count_trigrams = zip(*top_trigrams)\n",
        "\n",
        "#converting lists of strings out of ngrams\n",
        "list_of_strings_unigrams = []\n",
        "list_of_strings_bigrams = []\n",
        "list_of_strings_trigrams = []\n",
        "\n",
        "for s in strings_unigrams:\n",
        "    for st in s:\n",
        "        list_of_strings_unigrams.append(st)\n",
        "\n",
        "list_of_strings_bigrams = [str(bi_strings) for bi_strings in strings_bigrams]\n",
        "list_of_strings_trigrams = [str(tri_strings) for tri_strings in strings_trigrams]\n",
        "\n",
        "plt.figure(1, figsize=(10, 8))\n",
        "plt.title('Top Unigrams')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Unigrams')\n",
        "plt.barh(list_of_strings_unigrams, count_unigrams)\n",
        "\n",
        "plt.figure(2, figsize=(10, 8))\n",
        "plt.title('Top Bigrams')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Bigrams')\n",
        "plt.barh(list_of_strings_bigrams, count_bigrams)\n",
        "\n",
        "plt.figure(3, figsize=(10, 8))\n",
        "plt.title('Top Trigrams')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Trigrams')\n",
        "plt.barh(list_of_strings_trigrams, count_trigrams)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YwBwieZ8yRw",
        "colab_type": "text"
      },
      "source": [
        "##Visualizing the GPEs\n",
        "\n",
        "we will visualize the Geopolitical Entities present across the articles. Like top 10 GPEs mentioned across all the articles.\n",
        "\n",
        "It is observed that the business news articles contain US the most. Apart from US, others such as UK, Chine, India, Japan are mentioned more frequently.\n",
        "\n",
        "Most of the business news are related to these countires, while other african and american countries are covered less.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kctOVaw8wfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPE_counts = {}\n",
        "\n",
        "def Count_GPEs(text):\n",
        "    global GPE_counts;\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'GPE':\n",
        "            if ent.text not in GPE_counts.keys():\n",
        "                GPE_counts[ent.text] = 1\n",
        "            else:\n",
        "                GPE_counts[ent.text] += 1\n",
        "\n",
        "\n",
        "for article in df_articles['lines_removed']:\n",
        "    Count_GPEs(article)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhar6fIvKRNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting the top GPE counts\n",
        "top_GPEs = sorted(GPE_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "top_GPEs[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1p0LorLuFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unzip the keys and values\n",
        "GPEs, counts = zip(*top_GPEs[:20])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title('Top Geographical Places')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Places')\n",
        "plt.barh(GPEs, counts)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlnmW0f9RgCC",
        "colab_type": "text"
      },
      "source": [
        "##Let's generate the word cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH0Kk4X0Nx_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from matplotlib import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_articles = ' '.join(df_articles['stops_removed'].str.lower())\n",
        "\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=500).generate(all_articles)\n",
        "\n",
        "rcParams['figure.figsize'] = 15, 10\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp_4ulaM0oyS",
        "colab_type": "text"
      },
      "source": [
        "##And now the Summary Part.\n",
        "\n",
        "For summary, we calculate the word frequencies from the article and then calculate the sentence score by summing the frequencies of words present in it. If the length of a sentence is more than 30 words, we won't consider it to be in the summary.\n",
        "\n",
        "At last we take the 7 highest scored sentences out of all to make the summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAAuIP9rZjpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import heapq\n",
        "\n",
        "def generate_summary(text_with_periods, cleaned_text):\n",
        "    sample_text = text_with_periods\n",
        "    doc = nlp(sample_text)\n",
        "    sentence_list = []\n",
        "\n",
        "    for idx, sent in enumerate(doc.sents):\n",
        "        sentence_list.append(re.sub(r'[^\\w\\s]', '', str(sent)))\n",
        "\n",
        "    #calculate word frequencies\n",
        "    word_freqs = {}\n",
        "    for word in nlp(cleaned_text, disable=['parser', 'tagger', 'ner']):\n",
        "        if word.text not in word_freqs.keys():\n",
        "            word_freqs[word.text] = 1\n",
        "        else:\n",
        "            word_freqs[word.text] += 1\n",
        "\n",
        "    max_freq = max(word_freqs.values())\n",
        "\n",
        "    #weighted frequency:\n",
        "    for word in word_freqs.keys():\n",
        "        word_freqs[word] /= max_freq\n",
        "\n",
        "    #calculate sentence scores\n",
        "    sent_scores = {}\n",
        "    for sent in sentence_list:\n",
        "        for word in nlp(sent.lower(), disable=['tagger','parser','ner']):\n",
        "            if word.text in word_freqs.keys():\n",
        "                if len(sent.split(' ')) < 30:\n",
        "                    if sent not in sent_scores.keys():\n",
        "                        sent_scores[sent] = word_freqs[word.text]\n",
        "                    else:\n",
        "                        sent_scores[sent] += word_freqs[word.text]\n",
        "\n",
        "    summary_sentences = heapq.nlargest(5, sent_scores, key= sent_scores.get)\n",
        "\n",
        "    summary = '. '.join(summary_sentences)\n",
        "    print(\"Original Text::::::::::::\\n\")\n",
        "    print(text_with_periods)\n",
        "    print('\\n\\nSummarized text::::::::\\n')\n",
        "    print(summary)\n",
        "\n",
        "    return summary;\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CmpsI6Xj1FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "system_summary = generate_summary(df_articles['cleaned_up_text'][10], df_articles['stops_removed'][10])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}